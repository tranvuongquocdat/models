{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import datetime\n",
    "from ultralytics import YOLO\n",
    "import collections\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "# Load the pre-trained YOLOv8n model\n",
    "model = YOLO(r\"models\\yolov8n_sign_language.pt\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Initialize MediaPipe drawing utils for drawing hands on the image\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def draw_detections(frame, last_detections):\n",
    "    CONFIDENCE_THRESHOLD = 0.7\n",
    "    COLOR = (153, 255, 204)\n",
    "    result = 99\n",
    "    if last_detections is not None:\n",
    "        for data in last_detections.boxes.data.tolist():\n",
    "            confidence = data[4]\n",
    "            if float(confidence) < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "            xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), COLOR, 2)\n",
    "            class_id = data[5]\n",
    "            result = class_id\n",
    "            text = f\"{class_id}, {confidence:.2f}\"\n",
    "            cv2.putText(frame, text, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR, 2)\n",
    "    return frame, result\n",
    "\n",
    "def numpy_array_to_string(arr):\n",
    "    return ' '.join(map(str, arr))\n",
    "\n",
    "def put_vietnamese_text(img, text, position, font_path, font_size, color):\n",
    "    # Chuyển đổi hình ảnh từ OpenCV sang PIL\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "\n",
    "    # Chuyển đổi hình ảnh từ PIL sang OpenCV\n",
    "    img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "    return img\n",
    "\n",
    "label_map = {0: \"Toi\",\n",
    "             1: \"\",\n",
    "             2: \"com\",\n",
    "             3: \"\",\n",
    "             4: \"\",\n",
    "             5: \"\",\n",
    "             6: \"\",\n",
    "             7: \"\",\n",
    "             8: \"\",\n",
    "             9: \"\",\n",
    "             10: \"ban\",\n",
    "             11: \"\",\n",
    "             12: \"\",\n",
    "             13: \"\",\n",
    "             14: \"an\",\n",
    "             15: \"\",\n",
    "             16: \"\",\n",
    "             17: \"\",\n",
    "             18: \"\",\n",
    "             19: \"\",\n",
    "             20: \"di\",\n",
    "             21: \"choi\",\n",
    "             22: \"chao\",\n",
    "             23: \"\",\n",
    "             24: \"\",\n",
    "             25: \"\",\n",
    "             26: \"\",\n",
    "             27: \"\",\n",
    "             28: \"\",\n",
    "             29: \"\",\n",
    "             30: \"\",\n",
    "             }\n",
    "\n",
    "prev_time = datetime.datetime.now()\n",
    "\n",
    "# Create a VideoCapture object to capture the video from your webcam\n",
    "id = 0\n",
    "cap = cv2.VideoCapture(id)\n",
    "\n",
    "sign_arr = np.empty(0)\n",
    "pred_count = 0\n",
    "result_arr = []\n",
    "max_empty_hand_frame = 30\n",
    "empty_hand_frame = 0\n",
    "current_result = 88\n",
    "font_path = \"Disney.ttf\"\n",
    "result = 0\n",
    "\n",
    "# Define a deque to store the last N results for smoothing\n",
    "N = 10  # Size of the sliding window\n",
    "result_buffer = collections.deque(maxlen=N)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert the frame color from BGR to RGB\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to detect hands\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        empty_hand_frame = 0\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            x_min = w\n",
    "            x_max = 0\n",
    "            y_min = h\n",
    "            y_max = 0\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "\n",
    "            expand_width = int((x_max - x_min) * 0.8)\n",
    "            expand_height = int((y_max - y_min) * 0.8)\n",
    "            x_min = max(0, x_min - expand_width)\n",
    "            x_max = min(w, x_max + expand_width)\n",
    "            y_min = max(0, y_min - expand_height)\n",
    "            y_max = min(h, y_max + expand_height)\n",
    "\n",
    "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
    "            detections = model(cropped_image)[0]\n",
    "            cropped_image, result = draw_detections(cropped_image, detections)\n",
    "\n",
    "            if result != 99:\n",
    "                result_buffer.append(result)\n",
    "\n",
    "            cv2.imshow(\"Image\", cropped_image)\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    else:\n",
    "        empty_hand_frame += 1\n",
    "\n",
    "    if len(result_buffer) > 0:\n",
    "        # Use the most common element in the buffer as the stable result\n",
    "        result = max(set(result_buffer), key=result_buffer.count)\n",
    "        if result != current_result:\n",
    "            sign_arr = np.append(sign_arr, label_map[result])\n",
    "            current_result = result\n",
    "\n",
    "    if empty_hand_frame == max_empty_hand_frame:\n",
    "        if result == 0:\n",
    "            current_result = 88\n",
    "        else:\n",
    "            current_result = result\n",
    "        sign_arr = np.empty(0)\n",
    "    \n",
    "    curr_time = datetime.datetime.now()\n",
    "    delta_time = curr_time - prev_time\n",
    "    fps = 1 / delta_time.total_seconds()\n",
    "    prev_time = curr_time\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    FPS_COLOR = (153, 255, 204)\n",
    "    fps_text = f\"FPS: {fps:.2f}\"\n",
    "    cv2.putText(frame, fps_text, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, FPS_COLOR, 2)\n",
    "\n",
    "    # Create a separate window for displaying the results\n",
    "    result_window = np.zeros((80, 1000, 3), dtype=np.uint8)\n",
    "    show_result = numpy_array_to_string(sign_arr)\n",
    "    cv2.putText(result_window, str(show_result), (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Hand Detection', frame)\n",
    "    cv2.imshow(\"Result window\", result_window)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import datetime\n",
    "from ultralytics import YOLO\n",
    "import collections\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "# Load the pre-trained YOLOv8n model\n",
    "model = YOLO(r\"runs\\detect\\train9\\weights\\best.pt\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Thiết lập socket client\n",
    "client_socket = socket.socket()\n",
    "client_socket.connect(('172.20.10.12', 8000))\n",
    "\n",
    "# Nhận dữ liệu hình ảnh từ server\n",
    "data = b\"\"\n",
    "payload_size = struct.calcsize(\"<L\")\n",
    "\n",
    "# Initialize MediaPipe drawing utils for drawing hands on the image\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def draw_detections(frame, last_detections):\n",
    "    CONFIDENCE_THRESHOLD = 0.7\n",
    "    COLOR = (153, 255, 204)\n",
    "    result = 99\n",
    "    if last_detections is not None:\n",
    "        for data in last_detections.boxes.data.tolist():\n",
    "            confidence = data[4]\n",
    "            if float(confidence) < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "            xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), COLOR, 2)\n",
    "            class_id = data[5]\n",
    "            result = class_id\n",
    "            text = f\"{class_id}, {confidence:.2f}\"\n",
    "            cv2.putText(frame, text, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR, 2)\n",
    "    return frame, result\n",
    "\n",
    "def numpy_array_to_string(arr):\n",
    "    return ' '.join(map(str, arr))\n",
    "\n",
    "def put_vietnamese_text(img, text, position, font_path, font_size, color):\n",
    "    # Chuyển đổi hình ảnh từ OpenCV sang PIL\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "\n",
    "    # Chuyển đổi hình ảnh từ PIL sang OpenCV\n",
    "    img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "    return img\n",
    "\n",
    "label_map = {0: \"Toi\",\n",
    "             1: \"\",\n",
    "             2: \"com\",\n",
    "             3: \"\",\n",
    "             4: \"\",\n",
    "             5: \"\",\n",
    "             6: \"\",\n",
    "             7: \"\",\n",
    "             8: \"\",\n",
    "             9: \"\",\n",
    "             10: \"ban\",\n",
    "             11: \"\",\n",
    "             12: \"\",\n",
    "             13: \"\",\n",
    "             14: \"an\",\n",
    "             15: \"\",\n",
    "             16: \"\",\n",
    "             17: \"\",\n",
    "             18: \"\",\n",
    "             19: \"\",\n",
    "             20: \"di\",\n",
    "             21: \"choi\",\n",
    "             22: \"chao\",\n",
    "             23: \"\",\n",
    "             24: \"\",\n",
    "             25: \"\",\n",
    "             26: \"\",\n",
    "             27: \"\",\n",
    "             28: \"\",\n",
    "             29: \"\",\n",
    "             30: \"\",\n",
    "             }\n",
    "\n",
    "prev_time = datetime.datetime.now()\n",
    "\n",
    "sign_arr = np.empty(0)\n",
    "pred_count = 0\n",
    "result_arr = []\n",
    "max_empty_hand_frame = 30\n",
    "empty_hand_frame = 0\n",
    "current_result = 88\n",
    "font_path = \"Disney.ttf\"\n",
    "result = 0\n",
    "\n",
    "# Define a deque to store the last N results for smoothing\n",
    "N = 10  # Size of the sliding window\n",
    "result_buffer = collections.deque(maxlen=N)\n",
    "\n",
    "while True:\n",
    "    while len(data) < payload_size:\n",
    "        data += client_socket.recv(4096)\n",
    "\n",
    "    packed_msg_size = data[:payload_size]\n",
    "    data = data[payload_size:]\n",
    "    msg_size = struct.unpack(\"<L\", packed_msg_size)[0]\n",
    "\n",
    "    while len(data) < msg_size:\n",
    "        data += client_socket.recv(4096)\n",
    "\n",
    "    frame_data = data[:msg_size]\n",
    "    data = data[msg_size:]\n",
    "\n",
    "    # Giải mã hình ảnh\n",
    "    frame = cv2.imdecode(np.frombuffer(frame_data, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Convert the frame color from BGR to RGB\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to detect hands\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        empty_hand_frame = 0\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            x_min = w\n",
    "            x_max = 0\n",
    "            y_min = h\n",
    "            y_max = 0\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "\n",
    "            expand_width = int((x_max - x_min) * 0.8)\n",
    "            expand_height = int((y_max - y_min) * 0.8)\n",
    "            x_min = max(0, x_min - expand_width)\n",
    "            x_max = min(w, x_max + expand_width)\n",
    "            y_min = max(0, y_min - expand_height)\n",
    "            y_max = min(h, y_max + expand_height)\n",
    "\n",
    "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
    "            # cropped_image = frame\n",
    "            detections = model(cropped_image)[0]\n",
    "            cropped_image, result = draw_detections(cropped_image, detections)\n",
    "\n",
    "            if result != 99:\n",
    "                result_buffer.append(result)\n",
    "\n",
    "            cv2.imshow(\"Image\", cropped_image)\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    else:\n",
    "        empty_hand_frame += 1\n",
    "\n",
    "    if len(result_buffer) > 0:\n",
    "        # Use the most common element in the buffer as the stable result\n",
    "        result = max(set(result_buffer), key=result_buffer.count)\n",
    "        if result != current_result:\n",
    "            sign_arr = np.append(sign_arr, label_map[result])\n",
    "            current_result = result\n",
    "\n",
    "    if empty_hand_frame == max_empty_hand_frame:\n",
    "        if result == 0:\n",
    "            current_result = 88\n",
    "        else:\n",
    "            current_result = result\n",
    "        sign_arr = np.empty(0)\n",
    "    \n",
    "    curr_time = datetime.datetime.now()\n",
    "    delta_time = curr_time - prev_time\n",
    "    fps = 1 / delta_time.total_seconds()\n",
    "    prev_time = curr_time\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    FPS_COLOR = (153, 255, 204)\n",
    "    fps_text = f\"FPS: {fps:.2f}\"\n",
    "    cv2.putText(frame, fps_text, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, FPS_COLOR, 2)\n",
    "\n",
    "    # Create a separate window for displaying the results\n",
    "    result_window = np.zeros((80, 1000, 3), dtype=np.uint8)\n",
    "    show_result = numpy_array_to_string(sign_arr)\n",
    "    cv2.putText(result_window, str(show_result), (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Hand Detection', frame)\n",
    "    cv2.imshow(\"Result window\", result_window)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "client_socket.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
